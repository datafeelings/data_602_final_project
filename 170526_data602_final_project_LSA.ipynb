{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='cuny.png' align='left'>\n",
    "# DATA 602 Semester Project: Latent Semantic Analysis\n",
    "\n",
    "Spring semester 2017  \n",
    "By: Dmitriy Vecheruk  \n",
    "\n",
    "  \n",
    "- - -\n",
    "  \n",
    "## 0. Introduction\n",
    "\n",
    "This interactive notebook is my submission for the CUNY MSDA DATA 602 Semester Project assignment. The goal of the assignment is to use the Python programming skils gained in the course to write a program that can analyse and visualize a dataset.  \n",
    "  \n",
    "**Project Motivation**\n",
    "  \n",
    "For my project, I have chosen to implement a Latent Semantic Analysis [1] on a corpus of English language documents with the aim of providing synonymous terms. \n",
    "My rationale is that the LSA approach appears very generic to me and can be also applied in the business context whenever other feature vectors are available (e.g. a vector of attributes per customer instead of a vector of words), and the resulting matrices can be applied flexibly to answer a number of questions on feature or document (or subject) similarity, context or importance.\n",
    "  \n",
    "**The dataset**\n",
    "  \n",
    "The dataset I have used is a moderately sized corpus of 2225 documents from the BBC news website (in English) corresponding to stories in five topical areas from 2004-2005: business, entertainment, politics, sport, and technology [2].\n",
    "  \n",
    "\n",
    "**Tools used**\n",
    "  \n",
    "In order for this notebook to run as intended, you should have the following Python modules installed:  \n",
    "  \n",
    "* `os`, `urllib2`, `zipfile` - for reading the corpus\n",
    "* `re`, `string` - for string processing\n",
    "* `numpy`,`pandas`, `sklearn` - for processing the data and running the LSA\n",
    "* `bokeh`, `IPython.html`, `ipywidgets` - for the interactive visualization of the results \n",
    "  \n",
    "**Implementation overview**\n",
    "  \n",
    "Here is an overview of the data processing flow and the components:\n",
    "\n",
    "<img src=\"app_struct.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Offline part\n",
    "\n",
    "This is the main part of the program. The offline part runs only once, and parses the corpus, runs the latent semantic analysis functions over it and produces the necessary outputs for the interactive search of similar terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Read and clean the corpus\n",
    "\n",
    "First we load the required modules and fetch the zipped corpus data [3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib2\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer,HashingVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if corpus file exists and if not - download it\n",
    "\n",
    "if not os.path.isfile('corpus.zip'):\n",
    "    print 'Downloading data from GitHub...'\n",
    "    \n",
    "    # Download the zip into the data/ directory\n",
    "    response = urllib2.urlopen('https://github.com/datafeelings/data_602_final_project/raw/master/data/bbc-fulltext.zip')\n",
    "    down_file= response.read()\n",
    "\n",
    "    with open(\"corpus.zip\", 'w') as f:\n",
    "        f.write(down_file)\n",
    "    print 'Saved \"corpus.zip\" in the notebook directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a corpus with 2225 documents\n"
     ]
    }
   ],
   "source": [
    "# Read files from zip\n",
    "arc_path = \"corpus.zip\"\n",
    "\n",
    "def read_corpus_from_zip(archive_path):\n",
    "    ''' Reads a corpus of .txt documents stored in subfolders within a .zip archive\n",
    "    \n",
    "    Args:\n",
    "        archive_path: path to the zipped corpus of .txt documents\n",
    "\n",
    "    Returns:\n",
    "        Dict with documents and terms\n",
    "    '''\n",
    "    \n",
    "    import zipfile\n",
    "    import os\n",
    "    \n",
    "    with zipfile.ZipFile(archive_path) as z:\n",
    "\n",
    "        docs = []\n",
    "        terms = []\n",
    "        # need to read only .txt files but not those hidden ones created by the MacOS\n",
    "\n",
    "        for filename in [f for f in z.namelist() if (not f.startswith('_'))& f.endswith('.txt')]:\n",
    "            with z.open(filename) as f:\n",
    "                docs.append(filename)\n",
    "                words = f.read().replace('\\n', '')\n",
    "                terms.append(words)\n",
    "\n",
    "    print 'Created a corpus with {} documents'.format(len(docs))\n",
    "    return {'docs' : docs,'terms' : terms}\n",
    "\n",
    "corpus = read_corpus_from_zip(arc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and stem the strings before passing to the tokenizer that will count term occurrences per document [3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process terms: Remove punctuation & numbers, lowercase, remove articles and short words\n",
    "\n",
    "def clean_strings(string_):\n",
    "    '''Removes punctuation & numbers, turns to lowercase, removes 1- or 2-letter terms from a string of text\n",
    "    \n",
    "    Args:\n",
    "        string_: (str) path to the zipped corpus of .txt documents\n",
    "\n",
    "    Returns:\n",
    "        (str) processed string\n",
    "    '''\n",
    "    \n",
    "    import string\n",
    "    import re\n",
    "    \n",
    "    string_ = string_.lower()  # lowercase\n",
    "    string_ = re.sub(r'[^\\x00-\\x7f]',r' ',string_) # remove non-unicode characters\n",
    "    string_ = re.sub(r'[\\d]',r'',string_) # retain only letters\n",
    "    string_ = re.sub(r'[%$()!.,]',r' ',string_) # retain only letters\n",
    "    string_ = re.sub(r'[ +]',r' ',string_) # collapse multiple whitespaces\n",
    "    string_ = re.sub(r' *\\b[a-zA-Z]{1,2}\\b',r'',string_) # remove all 1- or 2-word terms\n",
    "    \n",
    "    return string_\n",
    "\n",
    "\n",
    "corpus['terms'] = [clean_strings(item) for item in corpus['terms']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 300 symbols of the first document after parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sales boost time warner profitquarterly profits media giant timewarner jumped  for the three months december  from year-earlier the firm  which now one the biggest investors google  benefited from sales high-speed internet connections and higher advert sales  timewarner said fourth quarter sales ro'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 300 symbols of the first document\n",
    "corpus['terms'][0][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Run the LSA \n",
    "\n",
    "This part relies extensively on the LSA implementation in the Scikit-Learn module. The benefits of this approach vs. custom-implemented functions is the speed of processing. \n",
    "\n",
    "The main reference for this part is the following tutorial by Peter Prettenhofer and Lars Buitinck [5]:  \n",
    "  \n",
    "http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py  \n",
    "  \n",
    "Other references covering the theory of the LSA and topic modeling are [6,7,8]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a `TfidfVectorizer` constructs a term-document matrix from the cleaned corpus. In the cells of this matrix are the TF-IDF weights of each term for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.789057s\n",
      "n_samples: 2225, n_features: 16723\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.7,min_df=2, stop_words='english',use_idf=True)\n",
    "# max_df=0.7 means that the terms appearing than more in the 70% of the documents are ignored\n",
    "# min_df=2 means that the terms appearing in less than 2 documents are ignored\n",
    "\n",
    "X = vectorizer.fit_transform(corpus['terms'])\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the actual Singular Value Decomposition of the term-document matrix is calculated:\n",
    "  \n",
    "* The TDM is decomposed into the matrix X of the terms mapped to \"concepts\"(or semantic dimensions, or components), the square matrix S of singular values with \"concepts\" as rows/columns, the matrix Y of the \"concepts\" mapped to documents,  \n",
    "* Using SVD truncation (rank-lowering), the dimension of the matrices are reduced so that S is a 100x100 matrix of the top 100 semantic dimensions (the nuber 100 selected based on literature review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.310288s\n",
      "n_terms: 16723, n_components: 100\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "# n_components = 100 means that the matrix is reduced to 100 components (top 100 singular values)\n",
    "# n_iter = 7 - Number of iterations for randomized SVD solver. \n",
    "\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "# use transpose to generate a matrix for terms(features) instead of documents\n",
    "X = lsa.fit_transform(X.transpose()) \n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_terms: %d, n_components: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11322606, -0.08858838, -0.11481925, ...,  0.02483692,\n",
       "         0.0308393 ,  0.12903484],\n",
       "       [ 0.11673417, -0.10178572, -0.12229693, ..., -0.1090511 ,\n",
       "         0.06981183, -0.03725003],\n",
       "       [ 0.11532662, -0.10773052, -0.02530448, ...,  0.0298613 ,\n",
       "        -0.01788788,  0.03954373],\n",
       "       ..., \n",
       "       [ 0.24039674, -0.20552223, -0.23199312, ...,  0.01500166,\n",
       "         0.05132643, -0.03396719],\n",
       "       [ 0.13739699, -0.15670377, -0.19861743, ...,  0.01157532,\n",
       "         0.18498143, -0.20329173],\n",
       "       [ 0.09546642, -0.124781  , -0.11831286, ..., -0.00733596,\n",
       "        -0.03006833, -0.11851548]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16723, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a matrix **X** with M = 16.723 terms described by N = 100 components representing the \"semantic dimensions\".  \n",
    "We can:\n",
    "  \n",
    "1) Look at the terms that have the highest scores within each dimension  \n",
    "2) Calculate pairwise similarity between two term vectors (1xN each) using cosine distance \n",
    "  \n",
    "So for any term we can provide a set of e.g. top 10 synonyms using the top values from the similarity comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Extract the top terms per dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll provide some insight into what topics the 100 semantic dimensions inferred from the corpus describe.  \n",
    "First we fetch the names of the terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'aaa',\n",
       " u'aaas',\n",
       " u'aac',\n",
       " u'aaron',\n",
       " u'abacus',\n",
       " u'abandon',\n",
       " u'abandoned',\n",
       " u'abandoning',\n",
       " u'abandonment',\n",
       " u'abating']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The array of term names\n",
    "terms = vectorizer.get_feature_names()\n",
    "terms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find top terms per semantic dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.113226</td>\n",
       "      <td>-0.088588</td>\n",
       "      <td>-0.114819</td>\n",
       "      <td>-0.066678</td>\n",
       "      <td>-0.017645</td>\n",
       "      <td>0.028401</td>\n",
       "      <td>-0.020780</td>\n",
       "      <td>-0.293838</td>\n",
       "      <td>-0.176486</td>\n",
       "      <td>0.032807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>-0.064412</td>\n",
       "      <td>0.018931</td>\n",
       "      <td>0.109256</td>\n",
       "      <td>-0.063180</td>\n",
       "      <td>0.108095</td>\n",
       "      <td>0.077723</td>\n",
       "      <td>0.024837</td>\n",
       "      <td>0.030839</td>\n",
       "      <td>0.129035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.116734</td>\n",
       "      <td>-0.101786</td>\n",
       "      <td>-0.122297</td>\n",
       "      <td>-0.047617</td>\n",
       "      <td>-0.036581</td>\n",
       "      <td>0.017029</td>\n",
       "      <td>-0.023478</td>\n",
       "      <td>-0.291808</td>\n",
       "      <td>-0.139500</td>\n",
       "      <td>0.071898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013701</td>\n",
       "      <td>0.239071</td>\n",
       "      <td>-0.020271</td>\n",
       "      <td>-0.023135</td>\n",
       "      <td>-0.017255</td>\n",
       "      <td>-0.037946</td>\n",
       "      <td>-0.050270</td>\n",
       "      <td>-0.109051</td>\n",
       "      <td>0.069812</td>\n",
       "      <td>-0.037250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.113226 -0.088588 -0.114819 -0.066678 -0.017645  0.028401 -0.020780   \n",
       "1  0.116734 -0.101786 -0.122297 -0.047617 -0.036581  0.017029 -0.023478   \n",
       "\n",
       "         7         8         9     ...           90        91        92  \\\n",
       "0 -0.293838 -0.176486  0.032807    ...     0.020546 -0.064412  0.018931   \n",
       "1 -0.291808 -0.139500  0.071898    ...     0.013701  0.239071 -0.020271   \n",
       "\n",
       "         93        94        95        96        97        98        99  \n",
       "0  0.109256 -0.063180  0.108095  0.077723  0.024837  0.030839  0.129035  \n",
       "1 -0.023135 -0.017255 -0.037946 -0.050270 -0.109051  0.069812 -0.037250  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# semantic dimensions = components, are the columns, one row per each term\n",
    "# Convert to a Pandas df for easier operations\n",
    "terms_comps = pd.DataFrame(X) \n",
    "terms_comps.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract top-scoring terms from each dimension to get a feeling of what topics they describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to filter top n words per semantic dimension\n",
    "\n",
    "def get_top_x_term_df(inp_matrix, term_names, top_x):\n",
    "    ''' Returns a dataframe with top-scoring terms per semantic dimension\n",
    "    \n",
    "    Args:\n",
    "        inp_matrix: (DataFrame) Matrix mapping terms in rows to weights per semantic component in columns\n",
    "        term_names: (List) list with the vocabulary of the terms\n",
    "        top_x: (int) number of top-scoring terms per dimension to return\n",
    "\n",
    "    Returns:\n",
    "        (DataFrame) dataframe holding the term, the dimension and its score\n",
    "    '''\n",
    "        \n",
    "    # Convert into long pandas table and use built-in top-n function, then use the index of the top scoring \n",
    "    # terms to filter the melted term_component df\n",
    "    tc_matrix = inp_matrix.copy()\n",
    "    val_cols = tc_matrix.columns\n",
    "    tc_matrix['term'] = term_names\n",
    "    tc_melt = pd.melt(tc_matrix, id_vars=['term'], value_vars=list(val_cols),var_name='sem_dim', value_name='weight')\n",
    "    top_weights = tc_melt.groupby(['sem_dim'])['weight'].nlargest(top_x)\n",
    "    top_df = tc_melt.ix[top_weights.index.get_level_values(1),:]\n",
    "    return top_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sem_dim\n",
       "0    far, added, make, time, way, long, end, major,...\n",
       "1    labour, election, manifesto, conservative, pol...\n",
       "2    analyst, analysts, growing, market, according,...\n",
       "3    film, cinematography, nominated, actress, acto...\n",
       "4    uses, lets, streaming, downloadable, use, user...\n",
       "5    litigation, maintains, damages, jurisdiction, ...\n",
       "6    meaningless, embattled, yugansk, rosneft, pump...\n",
       "7    lock, scrum, forwards, grewcock, kicker, engla...\n",
       "8    league, club, defender, clubs, football, premi...\n",
       "9    russians, tool, windows, program, log, tools, ...\n",
       "Name: term, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms = get_top_x_term_df(terms_comps,terms, 10) # extract top 10 words per dimension\n",
    "sem_dim_descr = top_terms.groupby('sem_dim')['term'].apply(lambda x: \"%s\" % ', '.join(x)) # reference [9]\n",
    "sem_dim_descr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the first dimensions show terms related to the same topic, e.g. dimension 1 appears to be related to politics, dimension 2 to \"stock markets or business\", dimension 3 is about films and actors, 4 about internet usage, 5 about court and litigation, however, the dimension 0 is unclear when judged alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Calculate similarity between terms\n",
    "\n",
    "Now we can finally use the matrix of the terms mapped to the semantic dimensions to calculate the distance between a given term and each other term - and then pick the closest ones. Here, the Pandas and list documentation was used as reference [10,11]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate cosine distances from 1 vector to all the other vectors to find close terms\n",
    "\n",
    "def top_synonyms(tc_matrix,vocab,search_term,search_n):\n",
    "    ''' For a given term-component matrix and a search term, returns top n most similar terms\n",
    "    \n",
    "    Args:\n",
    "        tc_matrix: (DataFrame) Matrix mapping terms in rows to weights per semantic component in columns\n",
    "        vocab: list with all the terms in the vocabulary; the length must match tc_matrix row number\n",
    "        search_term: (str) string with search term\n",
    "        search_n: (int) number of most similar terms to return\n",
    "\n",
    "    Returns:\n",
    "        (List) List of tuples with (term,similarity) to the search term\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Preprocess user input\n",
    "    search_term = clean_strings(search_term).split(' ')[0] # preprocess the input and take only the first string\n",
    "    \n",
    "    try: \n",
    "        search_index = vocab.index(search_term)\n",
    "        term_vec = tc_matrix[search_index,:].reshape(1, -1) # vector for the search item\n",
    "    \n",
    "        # find distances to each other term, as the values are normalized can use linear kernel instead of cosine\n",
    "        term_dist = linear_kernel(term_vec, tc_matrix) \n",
    "\n",
    "        vals_max = np.sort(term_dist).take(range(-(search_n+1),0)) # top n+1 closest terms (as the term itself will be closest)\n",
    "        vals_max = vals_max[::-1] \n",
    "        terms_idx_max = [np.where(term_dist[0] == vals_max[idx])[0][0] for idx in range(0,vals_max.shape[0])] \n",
    "        terms_max = [terms[i] for i in terms_idx_max] \n",
    "        \n",
    "        output = zip(terms_max, vals_max)[1:] # exclude the matched term itelf from the output\n",
    "        \n",
    "        # remove duplicates and sort again\n",
    "        output = set(output) \n",
    "        output = sorted(output, key=lambda tup: tup[1])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    except:\n",
    "        print(\"Term not found in the vocabulary\")\n",
    "        return ('',0.0)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `top_synonyms` returns the similar terms using the matrix `X` and the vocabulary list `terms` defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'suvs', 0.84374154366331178),\n",
       " (u'saab', 0.84552522738624325),\n",
       " (u'vehicles', 0.84767965294607106),\n",
       " (u'carmaker', 0.84870587296567201),\n",
       " (u'lexus', 0.8531752060932396),\n",
       " (u'diesel', 0.87026015727237005),\n",
       " (u'marques', 0.89193622549154128),\n",
       " (u'cars', 0.9229350141782231),\n",
       " (u'marque', 0.93718056654147608),\n",
       " (u'motor', 0.95454353841950124)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = top_synonyms(X, terms, 'Car', 10)\n",
    "syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Online part\n",
    "\n",
    "This part is the interactive component that uses the functions and input variables defined in the first part to interactively retrieve similar terms based on user input [12,13]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Interactive chart with similar terms\n",
    "  \n",
    "First we define a plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"2f460f6e-f43e-4ab8-9c3d-4637af8e0b12\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"2f460f6e-f43e-4ab8-9c3d-4637af8e0b12\");\n",
       "      el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"2f460f6e-f43e-4ab8-9c3d-4637af8e0b12\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '2f460f6e-f43e-4ab8-9c3d-4637af8e0b12' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"2f460f6e-f43e-4ab8-9c3d-4637af8e0b12\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"2f460f6e-f43e-4ab8-9c3d-4637af8e0b12\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load graphics\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw a bar chart based on the input\n",
    "\n",
    "def top_syn_bar(search_term,search_n, min_score):\n",
    "    '''\n",
    "    WRITE DOCSTRING\n",
    "    '''\n",
    "    from bokeh.palettes import PuRd\n",
    "    from bokeh.charts.attributes import cat, color\n",
    "    from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label, NumeralTickFormatter\n",
    "    \n",
    "    syns = top_synonyms(X, terms, search_term,search_n) # get the data\n",
    "    \n",
    "    # Set up the data for the Bokeh bar chart\n",
    "    chart_data = pd.DataFrame(syns)\n",
    "    chart_data.columns = [\"term\",\"score\"]\n",
    "    chart_data = chart_data.sort_values('score') # sort desc order\n",
    "    chart_data['x_lp'] = 0 # add a column for x annotation position\n",
    "    chart_data = chart_data[chart_data['score'] > min_score] # remove non-relevant records\n",
    "     \n",
    "    y = list(chart_data['term'])\n",
    "    x = list(chart_data['score'])\n",
    "    \n",
    "    f_title = 'Top Related Terms for: '+ search_term\n",
    "    \n",
    "    p = figure(title=f_title,y_range=y)\n",
    "\n",
    "    p.hbar(y=y, height=0.5, left=0,right=x, \n",
    "           color=\"navy\")\n",
    "\n",
    "    p.xaxis.axis_label = \"Relative Term Similarity\"\n",
    "    p.xaxis[0].formatter = NumeralTickFormatter(format=\"0%\")\n",
    "    p.yaxis.visible = False\n",
    "\n",
    "    \n",
    "    labels = LabelSet(x='x_lp', y='term', text='term', level='glyph',text_color=\"white\",\n",
    "              x_offset=5, y_offset=-8, source=ColumnDataSource(chart_data), \n",
    "                      render_mode='canvas')\n",
    "    \n",
    "    p.add_layout(labels)\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactive output**\n",
    "  \n",
    "And then we define the interactive modification of the plotting function using input widgets.\n",
    "Below this block you should see three widgets and a horizontal bar chart displaying the similarity of the found terms to the provided input. The widgets allow you to modify:\n",
    "  \n",
    "1) The input term  \n",
    "2) The maximum number of the related terms that is returned  \n",
    "3) The minimum level of similarity (0-100%) that is used to filter the returned list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"bk-plotdiv\" id=\"c4e89adf-486e-4dc2-bde5-637689309411\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    var force = false;\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "      window._bokeh_is_loading = undefined;\n",
       "    }\n",
       "  \n",
       "  \n",
       "    \n",
       "    if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "      window._bokeh_timeout = Date.now() + 0;\n",
       "      window._bokeh_failed_load = false;\n",
       "    }\n",
       "  \n",
       "    var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "       \"<div style='background-color: #fdd'>\\n\"+\n",
       "       \"<p>\\n\"+\n",
       "       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "       \"</p>\\n\"+\n",
       "       \"<ul>\\n\"+\n",
       "       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "       \"</ul>\\n\"+\n",
       "       \"<code>\\n\"+\n",
       "       \"from bokeh.resources import INLINE\\n\"+\n",
       "       \"output_notebook(resources=INLINE)\\n\"+\n",
       "       \"</code>\\n\"+\n",
       "       \"</div>\"}};\n",
       "  \n",
       "    function display_loaded() {\n",
       "      if (window.Bokeh !== undefined) {\n",
       "        var el = document.getElementById(\"c4e89adf-486e-4dc2-bde5-637689309411\");\n",
       "        el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "      } else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(display_loaded, 100)\n",
       "      }\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"c4e89adf-486e-4dc2-bde5-637689309411\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'c4e89adf-486e-4dc2-bde5-637689309411' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        (function() {\n",
       "          var fn = function() {\n",
       "            var docs_json = {\"76e2f85d-6139-46ce-9955-0921de9b085f\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"b3641eef-e244-42c2-8b6e-8cf94c71330a\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"format\":\"0%\"},\"id\":\"8661dccc-1fb7-409b-bb7d-49ed55aa5c77\",\"type\":\"NumeralTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"f799d00a-0f5a-483d-860f-093f0852296f\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"height\":{\"value\":0.5},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"3eb32a15-7e5e-4ee0-a496-5c0116c7e7c5\",\"type\":\"HBar\"},{\"attributes\":{},\"id\":\"e341b990-788c-4305-8095-adf69533a488\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"dbd34b7f-51bb-40f9-853b-d294b3dae95c\",\"type\":\"BasicTicker\"}},\"id\":\"cac08bb1-57f9-4459-8a24-a199dcce8c55\",\"type\":\"Grid\"},{\"attributes\":{\"below\":[{\"id\":\"2d157d40-dee8-412b-8df5-d21e96f7dc94\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"825a7854-68c8-4b57-a02c-c1e5822be7ed\",\"type\":\"CategoricalAxis\"}],\"renderers\":[{\"id\":\"2d157d40-dee8-412b-8df5-d21e96f7dc94\",\"type\":\"LinearAxis\"},{\"id\":\"cac08bb1-57f9-4459-8a24-a199dcce8c55\",\"type\":\"Grid\"},{\"id\":\"825a7854-68c8-4b57-a02c-c1e5822be7ed\",\"type\":\"CategoricalAxis\"},{\"id\":\"d7a3aa85-ff14-43b6-ac80-1f3bf73be257\",\"type\":\"Grid\"},{\"id\":\"3712b9e4-5462-4411-98c7-b8b6caa6bbc3\",\"type\":\"BoxAnnotation\"},{\"id\":\"c802add8-d574-4087-8733-5b462871a5da\",\"type\":\"GlyphRenderer\"},{\"id\":\"062d1d34-2631-4427-9a03-62909566e076\",\"type\":\"LabelSet\"}],\"title\":{\"id\":\"305ddc7c-c376-4c24-8a8d-e310adcaeb3b\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"b1b9ff5b-005c-4d66-ad69-4053561abb46\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"142d876a-2db8-4f0c-a696-47c5e493b38f\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"0ed47601-7ec0-4c8b-9c59-e62e52b16625\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"50e20535-8ad5-42b8-92c3-e6221956df7e\",\"type\":\"FactorRange\"}},\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"fill_color\":{\"value\":\"navy\"},\"height\":{\"value\":0.5},\"line_color\":{\"value\":\"navy\"},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"483c6c04-0739-4a50-9880-b15ade3c1bc9\",\"type\":\"HBar\"},{\"attributes\":{\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"370b3714-d3b0-40fa-8b05-8fcd720ae0e8\",\"type\":\"PanTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"3712b9e4-5462-4411-98c7-b8b6caa6bbc3\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"callback\":null},\"id\":\"0ed47601-7ec0-4c8b-9c59-e62e52b16625\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"dbd34b7f-51bb-40f9-853b-d294b3dae95c\",\"type\":\"BasicTicker\"},{\"attributes\":{\"plot\":null,\"text\":\"Top Related Terms for: chelsea\"},\"id\":\"305ddc7c-c376-4c24-8a8d-e310adcaeb3b\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"297bc7f8-6f1e-4b7a-82c1-7e499e127054\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"483c6c04-0739-4a50-9880-b15ade3c1bc9\",\"type\":\"HBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"3eb32a15-7e5e-4ee0-a496-5c0116c7e7c5\",\"type\":\"HBar\"},\"selection_glyph\":null},\"id\":\"c802add8-d574-4087-8733-5b462871a5da\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"370b3714-d3b0-40fa-8b05-8fcd720ae0e8\",\"type\":\"PanTool\"},{\"id\":\"c9fd1bd3-c160-432c-b420-61bf229668ab\",\"type\":\"WheelZoomTool\"},{\"id\":\"b2d6348a-dc60-4814-aa0a-a8d922eea284\",\"type\":\"BoxZoomTool\"},{\"id\":\"f799d00a-0f5a-483d-860f-093f0852296f\",\"type\":\"SaveTool\"},{\"id\":\"3b31094d-d1f3-480b-b820-bf85cc997378\",\"type\":\"ResetTool\"},{\"id\":\"da31d55f-24ab-40e7-b24c-25ea488d916a\",\"type\":\"HelpTool\"}]},\"id\":\"142d876a-2db8-4f0c-a696-47c5e493b38f\",\"type\":\"Toolbar\"},{\"attributes\":{\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"c9fd1bd3-c160-432c-b420-61bf229668ab\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"index\",\"term\",\"score\",\"x_lp\"],\"data\":{\"index\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"score\":{\"__ndarray__\":\"Cy76yhXE6j90ywLNktXqPwO9H/Yh5+o/iOboAPUF6z/0J2EnvArrP5iPn9itIes/e8X/tfZn6z922U7HYYDrP3KwF7Zuhes/cCFLELG56z8Y8p7xbdnrPzox10naZew/g2I0psk67T/NaCfsBVXtP/9AKpFhpu0/\",\"dtype\":\"float64\",\"shape\":[15]},\"term\":[\"crushed\",\"honeymoon\",\"barcelona\",\"tapped\",\"conceding\",\"porto\",\"gallas\",\"nou\",\"didier\",\"rinaldi\",\"serie\",\"bridge\",\"carling\",\"mourinho\",\"stamford\"],\"x_lp\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}},\"id\":\"b9ab9a7c-01b9-4f4b-ac0a-f3f72e2f7872\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"level\":\"glyph\",\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"source\":{\"id\":\"b9ab9a7c-01b9-4f4b-ac0a-f3f72e2f7872\",\"type\":\"ColumnDataSource\"},\"text\":{\"field\":\"term\"},\"text_color\":{\"value\":\"white\"},\"x\":{\"field\":\"x_lp\"},\"x_offset\":{\"value\":5},\"y\":{\"field\":\"term\"},\"y_offset\":{\"value\":-8}},\"id\":\"062d1d34-2631-4427-9a03-62909566e076\",\"type\":\"LabelSet\"},{\"attributes\":{},\"id\":\"b1b9ff5b-005c-4d66-ad69-4053561abb46\",\"type\":\"ToolEvents\"},{\"attributes\":{\"axis_label\":\"Relative Term Similarity\",\"formatter\":{\"id\":\"8661dccc-1fb7-409b-bb7d-49ed55aa5c77\",\"type\":\"NumeralTickFormatter\"},\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"dbd34b7f-51bb-40f9-853b-d294b3dae95c\",\"type\":\"BasicTicker\"}},\"id\":\"2d157d40-dee8-412b-8df5-d21e96f7dc94\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"right\"],\"data\":{\"right\":[0.8364361729049404,0.8385709766501876,0.8407144362553541,0.844477178360834,0.8450604218608064,0.8478612166331798,0.8564408831281719,0.8594216244444357,0.8600381428236814,0.8664174383384715,0.8702916831167071,0.8874331896018142,0.9134262319740433,0.9166288006192872,0.9265601954703299],\"y\":[\"crushed\",\"honeymoon\",\"barcelona\",\"tapped\",\"conceding\",\"porto\",\"gallas\",\"nou\",\"didier\",\"rinaldi\",\"serie\",\"bridge\",\"carling\",\"mourinho\",\"stamford\"]}},\"id\":\"297bc7f8-6f1e-4b7a-82c1-7e499e127054\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"e341b990-788c-4305-8095-adf69533a488\",\"type\":\"CategoricalTicker\"}},\"id\":\"d7a3aa85-ff14-43b6-ac80-1f3bf73be257\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"factors\":[\"crushed\",\"honeymoon\",\"barcelona\",\"tapped\",\"conceding\",\"porto\",\"gallas\",\"nou\",\"didier\",\"rinaldi\",\"serie\",\"bridge\",\"carling\",\"mourinho\",\"stamford\"]},\"id\":\"50e20535-8ad5-42b8-92c3-e6221956df7e\",\"type\":\"FactorRange\"},{\"attributes\":{\"overlay\":{\"id\":\"3712b9e4-5462-4411-98c7-b8b6caa6bbc3\",\"type\":\"BoxAnnotation\"},\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"b2d6348a-dc60-4814-aa0a-a8d922eea284\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"3b31094d-d1f3-480b-b820-bf85cc997378\",\"type\":\"ResetTool\"},{\"attributes\":{\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"da31d55f-24ab-40e7-b24c-25ea488d916a\",\"type\":\"HelpTool\"},{\"attributes\":{\"formatter\":{\"id\":\"b3641eef-e244-42c2-8b6e-8cf94c71330a\",\"type\":\"CategoricalTickFormatter\"},\"plot\":{\"id\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"e341b990-788c-4305-8095-adf69533a488\",\"type\":\"CategoricalTicker\"},\"visible\":false},\"id\":\"825a7854-68c8-4b57-a02c-c1e5822be7ed\",\"type\":\"CategoricalAxis\"}],\"root_ids\":[\"ae6b88c0-2094-4da3-a4e2-56860180d18e\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.5\"}};\n",
       "            var render_items = [{\"docid\":\"76e2f85d-6139-46ce-9955-0921de9b085f\",\"elementid\":\"c4e89adf-486e-4dc2-bde5-637689309411\",\"modelid\":\"ae6b88c0-2094-4da3-a4e2-56860180d18e\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "          };\n",
       "          if (document.readyState != \"loading\") fn();\n",
       "          else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "        })();\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      \n",
       "      if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "        for (var i = 0; i < inline_js.length; i++) {\n",
       "          inline_js[i](window.Bokeh);\n",
       "        }if (force === true) {\n",
       "          display_loaded();\n",
       "        }} else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(run_inline_js, 100);\n",
       "      } else if (!window._bokeh_failed_load) {\n",
       "        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "        window._bokeh_failed_load = true;\n",
       "      } else if (force !== true) {\n",
       "        var cell = $(document.getElementById(\"c4e89adf-486e-4dc2-bde5-637689309411\")).parents('.cell').data().cell;\n",
       "        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "      }\n",
       "  \n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.html.widgets import interact, interactive, fixed\n",
    "from IPython.html import widgets\n",
    "from ipywidgets import HBox, Label, IntSlider\n",
    "\n",
    "def f(term, n_terms,min_score):\n",
    "    '''Provides interactive output of top_syn_bar() modifiable by widgets'''\n",
    "    try:\n",
    "        top_syn_bar(term,n_terms,min_score)\n",
    "    except:\n",
    "        print 'Term not found: Chart not available'\n",
    "\n",
    "\n",
    "# 'interact' allows the widgets to modify the output of the function defined as the first argument\n",
    "\n",
    "interact(f, term = widgets.Text(value='car',description='Search Term: '),\n",
    "         n_terms=widgets.IntSlider(min=-10,max=25,step=1,value=10,description='Number of related terms: '),\n",
    "         min_score=widgets.FloatSlider(min=0,max=1,step=0.05,value=0.6, readout_format='.0%',\n",
    "                                       description='Minimum similarity: '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.  Discussion\n",
    "\n",
    "By trying out a few keywords it becomes apparent that the performance of the similarity search depends strongly on the topical coverage of the corpus. As we know, the BBC article corpus used here covers 5 topical areas: business, entertainment, politics, sport, and technology. Keywords from these areas such as \"money\", \"car\", \"Nirvana\", \"Chelsea\" (for the soccer team) or \"console\" subjectively seem to perform better (relevant terms are provided) than the terms outside of the topical scope of the corpus (e.g. \"pasta\", \"tomato\" - are not in the filtered vocabulary, and \"heat\" only provides \"synonyms\" related to overheating in supercomputers). \n",
    "  \n",
    "Therefore, this analysis could be repeated on a larger corpus in the future to get more precise and relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "  \n",
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "0. [Wikipedia: Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)  \n",
    "  \n",
    "1. [D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.](http://mlg.ucd.ie/datasets/bbc.html)\n",
    "  \n",
    "2. [Stackoverflow: How to download a ZIP file from a site Python](https://stackoverflow.com/questions/16760992/how-to-download-a-zip-file-from-a-site-python)  \n",
    "\n",
    "3. [Python: 7.2. re â€” Regular expression operations](https://docs.python.org/2/library/re.html)  \n",
    "\n",
    "4. [Clustering text documents using k-means](http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py)  \n",
    "\n",
    "5. [Cambridge University Press: Scoring, term weighting and the vector space model](https://nlp.stanford.edu/IR-book/html/htmledition/scoring-term-weighting-and-the-vector-space-model-1.html)\n",
    "\n",
    "6. [Mat Kelcey: latent semantic analysis via the singular value decomposition (for dummies)](http://matpalm.com/lsa_via_svd/)\n",
    "\n",
    "7. [William Webber: Lecture 7: Matrix decomposition and LSA](http://www.williamwebber.com/research/teaching/comp90042/2014s1/lect/l07.pdf)\n",
    "  \n",
    "8. [Stackoverflow: Pandas groupby: How to get a union of strings](https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings)\n",
    "  \n",
    "9. [pandas: powerful Python data analysis toolkit](http://pandas.pydata.org/pandas-docs/version/0.17.0/index.html)\n",
    "  \n",
    "10. [Stackoverflow: How to sort list/tuple of lists/tuples](https://stackoverflow.com/questions/3121979/how-to-sort-list-tuple-of-lists-tuples)\n",
    "   \n",
    "11. [Bokeh: User Guide](http://bokeh.pydata.org/en/latest/docs/user_guide.html#userguide)\n",
    "  \n",
    "12. [IPython: Using Interact](http://nbviewer.jupyter.org/github/quantopian/ipython/blob/master/examples/Interactive%20Widgets/Using%20Interact.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "a00090182b3e44d7a7b521043da6b8a4": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
